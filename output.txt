./main_relight.py:

import argparse
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
from video_dip.callbacks.image_logger import ImageLogger
from video_dip.models.modules.relight import RelightVDPModule
from video_dip.data.datamodule import VideoDIPDataModule
from video_dip.models.optical_flow import RAFT, RAFTModelSize, Farneback
from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping

def relight(
    learning_rate=2e-3,
    loss_weights=[1, .02],
    milestones=[5, 15, 45, 75],
    gamma=.5,
    warmup=True,
    input_path="datasets/relighting/outdoor_png/input/pair76",
    target_path="datasets/relighting/outdoor_png/GT/pair76",
    flow_path="flow_outputs",
    batch_size=4,
    num_workers=4,
    max_epochs=100,
    devices=[1],
    logger='tb',
    flow_model='raft',
    early_stopping=True
):
    # Initialize the model
    model = RelightVDPModule(
        learning_rate=learning_rate, 
        loss_weights=loss_weights,
        multi_step_scheduling_kwargs={
            'milestones': milestones,
            'gamma': gamma,
        },
        warmup=warmup
    )

    # Initialize the data module
    data_module = VideoDIPDataModule(
        input_path=input_path, 
        target_path=target_path,
        flow_path=flow_path,
        batch_size=batch_size, 
        num_workers=num_workers
    )

    if flow_model == 'raft':
        data_module.dump_optical_flow(flow_model=RAFT(RAFTModelSize.LARGE))
    elif flow_model == 'farneback':
        data_module.dump_optical_flow(flow_model=Farneback())
    else:
        raise ValueError(f"Invalid flow model: {flow_model}")

    # Initialize the loggers
    if logger == 'tb':
        logger = TensorBoardLogger("tb_logs", name=f"video_dip_relight_{input_path.split('/')[-1]}")
    elif logger == 'wandb':
        logger = WandbLogger(project="video_dip_relight")
        logger.watch(model)
    else:
        raise ValueError(f"Invalid logger: {logger}")
    
    callbacks = [
        ImageLogger(num_images=1),
        LearningRateMonitor(logging_interval='epoch')  # Log learning rate at every training step
    ]
    if early_stopping:
        callbacks.append(EarlyStopping(monitor='psnr', patience=20, mode='max'))

    # Initialize the trainer with the logger
    trainer = pl.Trainer(
        logger=logger, 
        devices=devices, 
        max_epochs=max_epochs, 
        callbacks=callbacks,
        benchmark=True,
        num_sanity_val_steps=0
    )

    # Fit the model
    trainer.fit(model, datamodule=data_module)

    return trainer.test(model, datamodule=data_module)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Relight video using RelightVDPModule.")
    parser.add_argument("--learning_rate", type=float, default=2e-3, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=2, type=float, default=[1, .02], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[5, 15, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--input_path", type=str, default="datasets/relight/input/pair76", help="Input path for the relighting videos.")
    parser.add_argument("--target_path", type=str, default="datasets/relight/GT/pair76", help="Target path for the ground truth videos.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs", help="Path to save the optical flow outputs.")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")
    parser.add_argument("--flow_model", type=str, choices=['raft', 'farneback'], default='raft', help="Optical flow model to use.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--early_stopping", type=bool, default=True, help="Early stopping flag for the model.")

    args = parser.parse_args()
    
    relight(
        learning_rate=args.learning_rate,
        loss_weights=args.loss_weights,
        milestones=args.milestones,
        gamma=args.gamma,
        input_path=args.input_path,
        target_path=args.target_path,
        flow_path=args.flow_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        max_epochs=args.max_epochs,
        devices=args.devices,
        logger=args.logger,
        flow_model=args.flow_model
    )


================================================================================

./benchmark_dehaze.py:

import os
import argparse
import numpy as np
from pathlib import Path
from main_dehaze import dehaze  # Import the dehazing function from the provided script

def benchmark_dehaze(dataset_path, **dehaze_kwargs):
    # Discover all sub-folders within the dataset folder
    sub_folders = [f.path for f in os.scandir(os.path.join(dataset_path, 'gt')) if f.is_dir()]

    print(f"Found {len(sub_folders)} sub-folders in the dataset.")

    all_metrics = []

    with open('dehaze_metrics.txt', 'w') as f:
        f.write('====================================================================================================\n')
        f.write(f"Found {len(sub_folders)} sub-folders in the dataset.\n")
        f.write('====================================================================================================\n')

    for sub_folder in sub_folders:
        print('====================================================================================================')
        print(f"Processing sub-folder: {sub_folder}")
        print('====================================================================================================')

        # Run the dehazing script for each sub-folder
        metrics = dehaze(
            input_path=sub_folder.replace('gt', 'hazy'),
            target_path=sub_folder,
            airlight_est_path=sub_folder.replace('gt', 'processed'),
            **dehaze_kwargs
        )

        all_metrics.append(metrics)

        with open('dehaze_metrics.txt', 'a') as f:
            f.write(f"Metrics for sub-folder: {sub_folder}\n")
            f.write(f"{metrics}\n")

    print("All Metrics:", all_metrics)

    # Average the metrics
    sums = {}
    counts = {}

    # Loop through all metrics and sum up the values for each field
    for metrics in all_metrics:
        for metric in metrics:
            for key, value in metric.items():
                if key not in sums:
                    sums[key] = 0
                    counts[key] = 0
                sums[key] += value
                counts[key] += 1

    # Calculate averages
    averaged_metrics = {key: sums[key] / counts[key] for key in sums}

    # Output the result
    print("Averaged Metrics:", averaged_metrics)

    with open('dehaze_metrics.txt', 'a') as f:
        f.write("Averaged Metrics:\n")
        f.write(f"{averaged_metrics}\n")

    return averaged_metrics

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Benchmark dehazing on multiple sub-folders.")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the dataset folder containing sub-folders.")
    parser.add_argument("--learning_rate", type=float, default=2e-3, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=2, type=float, default=[1, .02], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[5, 15, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs_dehaze", help="Path to save the optical flow outputs.")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")
    parser.add_argument("--flow_model", type=str, choices=['raft', 'farneback'], default='raft', help="Optical flow model to use.")

    args = parser.parse_args()
    
    dehaze_kwargs = {
        'learning_rate': args.learning_rate,
        'loss_weights': args.loss_weights,
        'milestones': args.milestones,
        'gamma': args.gamma,
        'warmup': args.warmup,
        'flow_path': args.flow_path,
        'batch_size': args.batch_size,
        'num_workers': args.num_workers,
        'max_epochs': args.max_epochs,
        'devices': args.devices,
        'logger': args.logger,
        'flow_model': args.flow_model
    }
    
    results = benchmark_dehaze(args.dataset_path, **dehaze_kwargs)
    print("Averaged Metrics:", results)


================================================================================

./benchmark_relight.py:

import os
import argparse
import numpy as np
from pathlib import Path
from main_relight import relight  # Import the relighting function from the provided script

def benchmark_relight(dataset_path, **relight_kwargs):
    # Discover all sub-folders within the dataset folder
    sub_folders = [f.path for f in os.scandir(os.path.join(dataset_path, 'GT')) if f.is_dir()]

    print(f"Found {len(sub_folders)} sub-folders in the dataset.")

    all_metrics = []

    with open('relight_metrics.txt', 'w') as f:
        f.write('====================================================================================================\n')
        f.write(f"Found {len(sub_folders)} sub-folders in the dataset.\n")
        f.write('====================================================================================================\n')

    for sub_folder in sub_folders:
        print('====================================================================================================')
        print(f"Processing sub-folder: {sub_folder}")
        print('====================================================================================================')

        # Run the relighting script for each sub-folder
        metrics = relight(
            input_path=sub_folder.replace('GT', 'input'),
            target_path=sub_folder,
            **relight_kwargs
        )

        all_metrics.append(metrics)

        with open('relight_metrics.txt', 'a') as f:
            f.write(f"Metrics for sub-folder: {sub_folder}\n")
            f.write(f"{metrics}\n")

    # Average the metrics
    sums = {}
    counts = {}

    # Loop through all metrics and sum up the values for each field
    for metrics in all_metrics:
        for metric in metrics:
            for key, value in metric.items():
                if key not in sums:
                    sums[key] = 0
                    counts[key] = 0
                sums[key] += value
                counts[key] += 1

    # Calculate averages
    averaged_metrics = {key: sums[key] / counts[key] for key in sums}
    print("All Metrics:", all_metrics)

    with open('relight_metrics.txt', 'a') as f:
        f.write(f"Averaged Metrics: {averaged_metrics}\n")
    
    return averaged_metrics

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Benchmark relighting on multiple sub-folders.")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the dataset folder containing sub-folders.")
    parser.add_argument("--learning_rate", type=float, default=5e-4, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=2, type=float, default=[1, .02], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[3, 5, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs", help="Path to save the optical flow outputs.")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")
    parser.add_argument("--flow_model", type=str, choices=['raft', 'farneback'], default='raft', help="Optical flow model to use.")

    args = parser.parse_args()
    
    relight_kwargs = {
        'learning_rate': args.learning_rate,
        'loss_weights': args.loss_weights,
        'milestones': args.milestones,
        'gamma': args.gamma,
        'warmup': args.warmup,
        'flow_path': args.flow_path,
        'batch_size': args.batch_size,
        'num_workers': args.num_workers,
        'max_epochs': args.max_epochs,
        'devices': args.devices,
        'logger': args.logger,
        'flow_model': args.flow_model
    }
    
    results = benchmark_relight(args.dataset_path, **relight_kwargs)
    print("Averaged Metrics:", results)


================================================================================

./output_all_code.py:

import os

def get_py_files_content(directory):
    py_files_content = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                with open(file_path, 'r') as f:
                    content = f.read()
                py_files_content.append((file_path, content))
    return py_files_content

def write_to_txt(output_file, py_files_content):
    with open(output_file, 'w') as f:
        for file_path, content in py_files_content:
            f.write(f"{file_path}:\n\n")
            f.write(content)
            f.write("\n\n" + "="*80 + "\n\n")

if __name__ == "__main__":
    directory = '.'
    output_file = "output.txt"
    py_files_content = get_py_files_content(directory)
    write_to_txt(output_file, py_files_content)
    print(f"Content written to {output_file}")


================================================================================

./setup.py:

from setuptools import setup, find_packages

setup(
    name='video_dip',
    version='0.1.0',
    author='Furkan & Alper',
    # author_email='your.email@example.com',
    description='A project for video deep image prior processing',
    long_description=open('README.md').read(),
    long_description_content_type='text/markdown',
    url='http://github.com/yourusername/video_dip',
    packages=find_packages(),
    include_package_data=True,
    classifiers=[
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',
        'Operating System :: OS Independent',
    ],
    python_requires='>=3.6',
    install_requires=[
        'torch',
        'numpy',
        # Add other dependencies required by your project
    ],
    entry_points={
        'console_scripts': [
            'video_dip=video_dip.module:main',
        ],
    },
)

================================================================================

./main_dehaze.py:

import argparse
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
from video_dip.callbacks.image_logger import ImageLogger
from video_dip.models.modules import DehazeVDPModule
from video_dip.data.datamodule import VideoDIPDataModule
from video_dip.models.optical_flow import RAFT, RAFTModelSize, Farneback
from pytorch_lightning.callbacks import LearningRateMonitor, EarlyStopping

def dehaze(
    learning_rate=2e-3,
    loss_weights=[1, .02],
    milestones=[5, 15, 45, 75],
    gamma=.5,
    warmup=True,
    input_path="datasets/dehazing/hazy/C005",
    target_path="datasets/dehazing/gt/C005",
    flow_path="flow_outputs",
    airlight_est_path="datasets/dehazing/processed/C005",
    batch_size=4,
    num_workers=4,
    max_epochs=100,
    devices=[1],
    logger='tb',
    flow_model='raft',
    early_stopping=True
):
    # Initialize the model
    model = DehazeVDPModule(
        learning_rate=learning_rate, 
        loss_weights=loss_weights,
        multi_step_scheduling_kwargs={
            'milestones': milestones,
            'gamma': gamma,
        },
        warmup=warmup
    )

    # Initialize the data module
    data_module = VideoDIPDataModule(
        input_path=input_path, 
        target_path=target_path,
        flow_path=flow_path,
        airlight_est_path=airlight_est_path,
        batch_size=batch_size, 
        num_workers=num_workers
    )

    if flow_model == 'raft':
        data_module.dump_optical_flow(flow_model=RAFT(RAFTModelSize.LARGE))
    elif flow_model == 'farneback':
        data_module.dump_optical_flow(flow_model=Farneback())
    else:
        raise ValueError(f"Invalid flow model: {flow_model}")

    # Initialize the loggers
    if logger == 'tb':
        logger = TensorBoardLogger("tb_logs", name=f"video_dip_dehaze_{input_path.split('/')[-1]}")
    elif logger == 'wandb':
        logger = WandbLogger(project="video_dip_dehaze")
        logger.watch(model)
    else:
        raise ValueError(f"Invalid logger: {logger}")

    callbacks = [
        ImageLogger(num_images=1),
        LearningRateMonitor(logging_interval='epoch')  # Log learning rate at every training step
    ]
    if early_stopping:
        callbacks.append(EarlyStopping(monitor='psnr', patience=20, mode='max'))

    # Initialize the trainer with the logger
    trainer = pl.Trainer(
        logger=logger, 
        devices=devices, 
        max_epochs=max_epochs, 
        callbacks=callbacks,
        benchmark=True,
        num_sanity_val_steps=0
    )

    # Fit the model
    trainer.fit(model, datamodule=data_module)

    return trainer.test(model, datamodule=data_module)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Dehaze video using DehazeVDPModule.")
    parser.add_argument("--learning_rate", type=float, default=2e-3, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=2, type=float, default=[1, .02], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[5, 15, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--input_path", type=str, default="datasets/dehazing/hazy/C005", help="Input path for the hazy videos.")
    parser.add_argument("--target_path", type=str, default="datasets/dehazing/gt/C005", help="Target path for the ground truth videos.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs", help="Path to save the optical flow outputs.")
    parser.add_argument("--airlight_est_path", type=str, default="datasets/dehazing/processed/C005", help="Path for airlight estimation data.")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=4, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")
    parser.add_argument("--flow_model", type=str, choices=['raft', 'farneback'], default='raft', help="Optical flow model to use.")
    parser.add_argument("--early_stopping", type=bool, default=True, help="Early stopping flag for the model.")

    args = parser.parse_args()
    
    dehaze(
        learning_rate=args.learning_rate,
        loss_weights=args.loss_weights,
        milestones=args.milestones,
        gamma=args.gamma,
        warmup=args.warmup,
        input_path=args.input_path,
        target_path=args.target_path,
        flow_path=args.flow_path,
        airlight_est_path=args.airlight_est_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        max_epochs=args.max_epochs,
        devices=args.devices,
        logger=args.logger,
        flow_model=args.flow_model
    )


================================================================================

./main_segmentation.py:

import argparse
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
from video_dip.callbacks.image_logger import ImageLogger
from video_dip.models.modules.segmentation import SegmentationVDPModule
from video_dip.data.datamodule import VideoDIPDataModule
from video_dip.models.optical_flow.raft import RAFT, RAFTModelSize
from pytorch_lightning.callbacks import LearningRateMonitor, StochasticWeightAveraging

def segment(
    learning_rate=1e-3,
    loss_weights=[.001, 1, 1, .001, .01],
    milestones=[5, 15, 45, 75],
    gamma=.5,
    warmup=True,
    input_path="datasets/input/bear",
    target_path="datasets/GT/pair1",
    flow_path="flow_outputs",
    batch_size=2,
    num_workers=8,
    max_epochs=100,
    devices=[1],
    logger='tb'
):
    # Initialize the model
    model = SegmentationVDPModule(
        learning_rate=learning_rate, 
        loss_weights=loss_weights,
        multi_step_scheduling_kwargs={
            'milestones': milestones,
            'gamma': gamma,
        },
        warmup=warmup
    )

    # Initialize the data module
    data_module = VideoDIPDataModule(
        input_path=input_path, 
        target_path=target_path,
        flow_path=flow_path,
        batch_size=batch_size, 
        num_workers=num_workers
    )

    # Initialize the loggers
    if logger == 'tb':
        logger = TensorBoardLogger("tb_logs", name="my_model")
    elif logger == 'wandb':
        logger = WandbLogger(project="video_dip_segmentation")
        logger.watch(model)
    else:
        raise ValueError(f"Invalid logger: {logger}")

    # Initialize the trainer with the logger
    trainer = pl.Trainer(
        logger=logger, 
        devices=devices, 
        max_epochs=max_epochs, 
        callbacks=[
            ImageLogger(num_images=1),
            LearningRateMonitor(logging_interval='epoch')  # Log learning rate at every training step
        ],
        benchmark=True,
    )

    # Fit the model
    trainer.fit(model, datamodule=data_module)

    return trainer.test(model, datamodule=data_module)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Segment video using SegmentationVDPModule.")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=5, type=float, default=[.001, 1, 1, .001, .01], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[5, 15, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--input_path", type=str, default="datasets/input/bear", help="Input path for the segmentation videos.")
    parser.add_argument("--target_path", type=str, default="datasets/GT/pair1", help="Target path for the ground truth videos.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs", help="Path to save the optical flow outputs.")
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=8, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")

    args = parser.parse_args()
    
    segment(
        learning_rate=args.learning_rate,
        loss_weights=args.loss_weights,
        milestones=args.milestones,
        gamma=args.gamma,
        warmup=args.warmup,
        input_path=args.input_path,
        target_path=args.target_path,
        flow_path=args.flow_path,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        max_epochs=args.max_epochs,
        devices=args.devices,
        logger=args.logger
    )


================================================================================

./benchmark_segmentation.py:

import os
import argparse
import numpy as np
from pathlib import Path
from main_segmentation import segment  # Import the segmentation function from the provided script

def benchmark_segment(dataset_path, **segment_kwargs):
    # Discover all sub-folders within the dataset folder
    sub_folders = [f.path for f in os.scandir(os.path.join(dataset_path, 'GT')) if f.is_dir()]

    print(f"Found {len(sub_folders)} sub-folders in the dataset.")

    all_metrics = []

    for sub_folder in sub_folders:
        print('====================================================================================================')
        print(f"Processing sub-folder: {sub_folder}")
        print('====================================================================================================')

        # Run the segmentation script for each sub-folder
        metrics = segment(
            input_path=sub_folder.replace('GT', 'input'),
            target_path=sub_folder,
            **segment_kwargs
        )

        all_metrics.append(metrics)

    # Average the metrics
    averaged_metrics = {key: np.mean([metric[key] for metric in all_metrics]) for key in all_metrics[0]}

    print("All Metrics:", all_metrics)
    
    return averaged_metrics

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Benchmark segmentation on multiple sub-folders.")
    parser.add_argument("--dataset_path", type=str, required=True, help="Path to the dataset folder containing sub-folders.")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate for the model.")
    parser.add_argument("--loss_weights", nargs=5, type=float, default=[.001, 1, 1, .001, .01], help="Loss weights for the model.")
    parser.add_argument("--milestones", nargs='+', type=int, default=[5, 15, 45, 75], help="Milestones for learning rate scheduling.")
    parser.add_argument("--gamma", type=float, default=.5, help="Gamma value for learning rate scheduling.")
    parser.add_argument("--warmup", type=bool, default=True, help="Warmup flag for the model.")
    parser.add_argument("--flow_path", type=str, default="flow_outputs", help="Path to save the optical flow outputs.")
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size for the data loader.")
    parser.add_argument("--num_workers", type=int, default=8, help="Number of workers for the data loader.")
    parser.add_argument("--max_epochs", type=int, default=100, help="Maximum number of epochs for training.")
    parser.add_argument("--devices", nargs='+', type=int, default=[1], help="Devices to use for training.")
    parser.add_argument("--logger", type=str, choices=['tb', 'wandb'], default='tb', help="Logger to use for training.")

    args = parser.parse_args()
    
    segment_kwargs = {
        'learning_rate': args.learning_rate,
        'loss_weights': args.loss_weights,
        'milestones': args.milestones,
        'gamma': args.gamma,
        'warmup': args.warmup,
        'flow_path': args.flow_path,
        'batch_size': args.batch_size,
        'num_workers': args.num_workers,
        'max_epochs': args.max_epochs,
        'devices': args.devices,
        'logger': args.logger
    }
    
    results = benchmark_segment(args.dataset_path, **segment_kwargs)
    print("Averaged Metrics:", results)


================================================================================

./video_dip/__init__.py:



================================================================================

./video_dip/data/dataset.py:

import os
import numpy as np
import torch
from torch.utils.data import Dataset
from glob import glob
from PIL import Image

IMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.JPG', '.JPEG', '.PNG', '.PPM', '.BMP', '.PGM', '.TIF']
VID_EXTENSIONS = ['.mp4', '.avi', '.mov', '.mkv']

class VideoDIPDataset(Dataset):
    """
    A custom dataset class for loading video frames or images for VideoDIP.

    Args:
        path (str): Path to the video file or directory containing images.
        target_path (str): Path to the directory containing the target frames or images.
        flow_path (str): Path to the directory containing the optical flow frames.
        transforms (torchvision.transforms.Compose): Transforms to be applied to the frames or images.       

    Raises:
        FileNotFoundError: If the specified path does not exist.
        ValueError: If the specified path is not a valid video file or does not contain any images.

    Attributes:
        frames (list): List of paths to the frames or images.

    Methods:
        default_transforms: Returns the default transforms to be applied to the frames or images.
    """

    def __init__(self, input_path, target_path = None, flow_path = None, transforms=None, airlight_est_path=None) -> None:
        assert input_path is not None, "Path to video file or images folder is required"
        assert isinstance(input_path, str), "Path must be a string"

        self.input_frames = self._get_frames(input_path)
        self.target_frames = self._get_frames(target_path) if target_path is not None else None
        self.optical_flow_frames = glob(os.path.join(flow_path, "*.npy")) if flow_path is not None else None
        if self.optical_flow_frames is not None:
            self.optical_flow_frames.sort()
        self.airlight_estimations = self._get_airlight_estimations(airlight_est_path) if airlight_est_path is not None else None

        self.transforms = transforms if transforms is not None else self.default_transforms()

    def _get_airlight_estimations(self, airlight_est):
        """
        Returns the paths to the airlight estimations in the specified directory.

        Args:
            airlight_est (str): Path to the directory containing the airlight estimations.

        Returns:
            list: List of paths to the airlight estimations.
        """

        if not os.path.exists(airlight_est):
            raise FileNotFoundError(f"Path {airlight_est} does not exist")
        
        airlight_estimations = glob(os.path.join(airlight_est, "*.csv"))
        airlight_estimations.sort()

        from pathlib import Path
        # Check if the files match with the input frames
        for frame, airlight in zip(self.input_frames, airlight_estimations):
            assert os.path.basename(airlight).startswith(Path(frame).stem), f"Air-light estimation file {airlight} does not match with the input frame {frame}"

        return airlight_estimations

    def _get_frames(self, path):
        """
        Returns the paths to the frames or images in the specified directory.

        Args:
            path (str): Path to the directory containing the frames or images.

        Returns:
            list: List of paths to the frames or images.
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Path {path} does not exist")
        elif os.path.isdir(path):
            frames = []
            for ext in IMG_EXTENSIONS:
                frames.extend(glob(os.path.join(path, f"*{ext}")))
                if len(frames) > 0:
                    break
            if len(frames) == 0:
                raise ValueError(f"Directory {path} does not contain images")
        elif os.path.isfile(path):
            if path.endswith(tuple(VID_EXTENSIONS)):
                frames = self._dump_video(path)
            else:
                raise ValueError(f"File {path} is not a valid video file")
        # sort the frames
        frames.sort()
        return frames       

    def _dump_video(self, path):
        """
        Dumps the frames of the video into a temporary directory.

        Args:
            path (str): Path to the video file.

        Returns:
            list: List of paths to the dumped frames.
        """
        import cv2
        import tempfile

        temp_dir = tempfile.mkdtemp()
        cap = cv2.VideoCapture(path)
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            frame_path = os.path.join(temp_dir, f"{len(frames)}.jpg")
            cv2.imwrite(frame_path, frame)
            frames.append(frame_path)
        cap.release()
        return frames

    def _load_image(self, path):
        """
        Loads an image from the specified path.

        Args:
            path (str): Path to the image file.

        Returns:
            PIL.Image.Image: The loaded image.
        """
        return Image.open(path).convert("RGB")

    @staticmethod
    def default_transforms():
        """
        Returns the default transforms to be applied to the frames or images.

        Returns:
            torchvision.transforms.Compose: The default transforms.
        """
        from torchvision import transforms

        # TODO: Probably, we want to normalize the input image respecting the VGG training 
        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((480, 856)),
            # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    @staticmethod
    def default_flow_transforms():
        """
        Returns the default transforms to be applied to the optical flow frames.

        Returns:
            torchvision.transforms.Compose: The default transforms.
        """
        from torchvision import transforms

        return transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((480, 856)),
        ])

    def __len__(self):
        if self.optical_flow_frames is None:
            return len(self.input_frames)
        return len(self.input_frames) - 1 # Omit the first two frames
    
    def __getitem__(self, idx):
        datum = {}
        if self.optical_flow_frames is not None:
            idx += 1 # Skip the first frame
            datum['flow'] = self.transforms(np.load(self.optical_flow_frames[idx - 1]))

        datum.update({
            "input": self.transforms(self._load_image(self.input_frames[idx])), 
            "filename": os.path.basename(self.input_frames[idx]),
            "prev_input": self.transforms(self._load_image(self.input_frames[idx - 1])) if idx > 0 else ""
        })
        if self.target_frames is not None:
            target_frame = self._load_image(self.target_frames[idx])
            datum["target"] = self.transforms(target_frame)

        if self.airlight_estimations is not None:
            datum["airlight"] = torch.tensor(np.genfromtxt(self.airlight_estimations[idx], delimiter=',')).float()

        return datum

if __name__ == '__main__':
    from torch.utils.data import DataLoader
    import PIL
    dip_dataset = VideoDIPDataset(
        "datasets/dehazing/hazy/C005", 
        "datasets/dehazing/gt/C005", 
        "flow_outputs",
        airlight_est_path="datasets/dehazing/processed/C005"
    )
    
    data_loader = DataLoader(dip_dataset, batch_size=2, num_workers=8)

    batch = next(iter(data_loader))
    print(batch.keys())

    print(batch['input'].shape)
    print(batch['target'].shape)
    print(batch['flow'].shape)
    print(batch['airlight'].shape)
    print(batch['filename'])

================================================================================

./video_dip/data/__init__.py:

from .dataset import VideoDIPDataset

# dipdataset = create_datset("video_dip/data/sora.mp4", target_resolution = (100,100))
# data_loader = DataLoader(dipdataset, batch_size=1, num_workers=8)
# img = next(iter(dipdataset))
# print(img.shape)
# visualize_frame(img)

================================================================================

./video_dip/data/datamodule.py:

import pytorch_lightning as pl

try:
    from video_dip.data.dataset import VideoDIPDataset
    from video_dip.models.optical_flow import Farneback
except ImportError:
    import sys
    import os
    # Add the parent of the parent directory to the path
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)))
    
    from video_dip.data.dataset import VideoDIPDataset
    from video_dip.models.optical_flow import Farneback

from torch.utils.data import DataLoader
import os

class VideoDIPDataModule(pl.LightningDataModule):
    def __init__(self, input_path, batch_size, num_workers, flow_path=None, target_path=None, airlight_est_path=None):
        """
        Initializes the VideoDIPDataModule.

        Args:
            input_path (str): Path to the input data.
            batch_size (int): Batch size for data loading.
            num_workers (int): Number of workers for data loading.
            target_path (str, optional): Path to the target data. Defaults to None.
            flow_path (str, optional): Path to the optical flow data. Defaults to None.
            airlight_est_path (str, optional): Path to the airlight estimation data. Defaults to None.
        """
        super().__init__()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.input_path = input_path
        self.target_path = target_path
        self.flow_path = flow_path
        self.airlight_est_path = airlight_est_path


    def dump_optical_flow(self, flow_model):
        """
        Dump the optical flow to a folder.

        Args:
            path (str, optional): Path to the folder to save the optical flow. Defaults to None.

        Returns:
            str: Path to the saved optical flow folder.
        """
        from tqdm.auto import tqdm
        import numpy as np

        flow_folder = self.flow_path
        if os.path.exists(flow_folder):
            import shutil
            shutil.rmtree(flow_folder)
        os.makedirs(flow_folder)
        
        dataset = VideoDIPDataset(
            self.input_path,
            transforms=VideoDIPDataset.default_flow_transforms()
        )
        
        for i in tqdm(range(1, len(dataset))):
            img1 = dataset[i - 1]['input']
            img2 = dataset[i]['input']
            base_name = dataset[i]['filename']
            flow = flow_model(img1, img2)
            # Save the flow as npy
            np.save(os.path.join(flow_folder, base_name), flow.transpose((1, 2, 0)))

            from torchvision.utils import flow_to_image, save_image
            import torch
            # Save the flow as image
            flow_image = flow_to_image(torch.tensor(flow)) / 255.0
            save_image(flow_image, os.path.join(flow_folder, base_name.replace('.npy', '.png')))
            
        return flow_folder

    def setup(self, stage=None):
        """
        Set up the data module.

        Args:
            stage (str, optional): Stage of the training. Defaults to None.
        """
        assert os.path.exists(self.flow_path), "The optical flow path does not exist. Did you forget to dump the optical flow before the training loop?"
        self.dataset = VideoDIPDataset(input_path=self.input_path, target_path=self.target_path, flow_path=self.flow_path, airlight_est_path=self.airlight_est_path)

    def train_dataloader(self):
        """
        Returns the data loader for training.

        Returns:
            torch.utils.data.DataLoader: Data loader for training.
        """
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers)
    
    def val_dataloader(self):
        """
        Returns the data loader for validation.

        Returns:
            torch.utils.data.DataLoader: Data loader for validation.
        """
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers)
    
    def test_dataloader(self):
        """
        Returns the data loader for testing.

        Returns:
            torch.utils.data.DataLoader: Data loader for testing.
        """
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers)
    
if __name__ == '__main__':
    from video_dip.models.optical_flow.raft import RAFT, RAFTModelSize

    module = VideoDIPDataModule("datasets/input/pair1", batch_size=2, num_workers=8, flow_model=Farneback())#RAFT(RAFTModelSize.LARGE))
    import os
    if os.path.exists("flow_outputs_2"):
        import shutil
        shutil.rmtree("flow_outputs_2")
    module.dump_optical_flow('flow_outputs_2')
    

================================================================================

./video_dip/metrics/pnsr.py:

from math import log10, sqrt 
import cv2 
import numpy as np 
import torch
import tqdm
from torch import Tensor
from torchmetrics import Metric
import torchvision
import torchvision.io as io

class PNSR(Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.add_state("num_calculations", default=torch.tensor(0), dist_reduce_fx="sum")
        self.add_state("cumulative_pnsr", default=torch.tensor(0.), dist_reduce_fx="sum")

    def update(self, original: Tensor, calculated: Tensor) -> None:

        """
            
        C,H,W Tensor

        """
        assert original.shape == calculated.shape, "Two inputs should be same size"

        mse = torch.mean((original.float() - calculated.float()) ** 2) 
        if(mse == 0): 
            pnsr = 100
        else:
            max_pixel = torch.as_tensor(255.0)
            pnsr = 20 * torch.log10(max_pixel / sqrt(mse)) 

        self.num_calculations += 1
        self.cumulative_pnsr += pnsr

    def compute(self) -> Tensor:
        return self.cumulative_pnsr.float() / self.num_calculations

def pnsr_video(video_path1, video_path2):
    """
    Calculates PNSR score over two videos
    """

    video1, audio1, info1 = io.read_video(video_path1, pts_unit='sec', output_format = "TCHW")
    video2, audio2, info2 = io.read_video(video_path2, pts_unit='sec', output_format = "TCHW")

    assert len(video1) == len(video2), "Given videos do not have equal number of frames"
    print(video1.shape)
    pnsr_metric = PNSR()
    for i in tqdm.tqdm(range(video1.shape[0])):
         pnsr_metric.update(video1[i,...], video2[i,...]*0.8)
    return pnsr_metric.compute()

def main(): 
    # original = cv2.imread(r"C:\Users\bahcekap\VideoDIP\video_dip\metrics\test\compressed_image.png", cv2.IMREAD_COLOR) 
    # compressed = cv2.imread(r"C:\Users\bahcekap\VideoDIP\video_dip\metrics\test\original_image.png", cv2.IMREAD_COLOR) 
    # original = torch.Tensor(original)
    # compressed = torch.Tensor(compressed)
    # pnsr_metric = PNSR() 
    # pnsr_metric.update(original, compressed)
    # value = pnsr_metric.compute()
     
    # print(f"PSNR value is {value} dB") 

    video_p = r"C:\Users\bahcekap\VideoDIP\video_dip\data\sora.mp4"

    value = pnsr_video( video_p, video_p)
    print(value)
	
if __name__ == "__main__": 
	main() 


================================================================================

./video_dip/metrics/ssim.py:

from skimage.metrics import structural_similarity as ssim
from torchmetrics import Metric

import cv2
import torch
import tqdm
from torch import Tensor
import torchvision
import torchvision.io as io

class SSIM(Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.add_state("num_calculations", default=torch.tensor(0), dist_reduce_fx="sum")
        self.add_state("cumulative_ssim", default=torch.tensor(0.), dist_reduce_fx="sum")

    def update(self, original: Tensor, calculated: Tensor) -> None:

        """
            
        C,H,W Tensor

        """
        assert original.shape == calculated.shape, "Two inputs should be same size"
        
        # cast to np
        origin_np = original.numpy()
        calculated_np = calculated.numpy()
        cumulative_ssim = 0
        # iterate over channels
        for c in range(origin_np.shape[0]): 
            temp_s = ssim(origin_np[c], calculated_np[c], data_range = 255.0)
            cumulative_ssim += temp_s

        self.num_calculations += 1
        self.cumulative_ssim += (cumulative_ssim / origin_np.shape[0])

    def compute(self) -> Tensor:
        return self.cumulative_ssim.float() / self.num_calculations



def ssim_video(video_path1, video_path2):
    """
    Calculates SSIM score over two videos
    """

    video1, audio1, info1 = io.read_video(video_path1, pts_unit='sec', output_format = "TCHW")
    video2, audio2, info2 = io.read_video(video_path2, pts_unit='sec', output_format = "TCHW")

    assert len(video1) == len(video2), "Given videos do not have equal number of frames"
    print(video1.shape)
    ssim_metric = SSIM()
    for i in tqdm.tqdm(range(len(video1))):
         ssim_metric.update(video1[i], video2[i])
    return ssim_metric.compute()
        
		
    


def main(): 
     # original = cv2.imread(r"C:\Users\bahcekap\VideoDIP\video_dip\metrics\test\compressed_image.png", cv2.IMREAD_COLOR) 
     # compressed = cv2.imread(r"C:\Users\bahcekap\VideoDIP\video_dip\metrics\test\original_image.png", cv2.IMREAD_COLOR) 
     # original = torch.as_tensor(original).permute((2,0,1))
     # compressed = torch.as_tensor(compressed).permute((2,0,1))
     # ssim_metric = SSIM()
     # ssim_metric.update(original, compressed)
     # ssim_metric.update(original, compressed*0.8)
     # value = ssim_metric.compute()
     # print(f"SSIM value is {value} ") 
    
    video_p = r"C:\Users\bahcekap\VideoDIP\video_dip\data\sora.mp4"
    ssim_score = ssim_video(video_p, video_p)
    print(ssim_score)
	
if __name__ == "__main__": 
	main() 




================================================================================

./video_dip/callbacks/image_logger.py:

import pytorch_lightning as pl
import torch
from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger
import torchvision

import numpy as np

class ImageLogger(pl.Callback):
    def __init__(self, num_images=4):
        super().__init__()
        self.num_images = num_images

    def on_validation_batch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule, outputs: torch.Tensor | torch.Dict[str, torch.Any] | None, batch: torch.Any, batch_idx: int, dataloader_idx: int = 0) -> None:
        num_total_batches = len(trainer.val_dataloaders)
        log_points = np.linspace(0, num_total_batches, self.num_images + 1, endpoint=False).astype(int)[1:] if self.num_images > 1 else [num_total_batches - 2]
        if batch_idx in log_points:
            flow_image = torchvision.utils.flow_to_image(batch['flow']) / 255.0
            if isinstance(trainer.logger, TensorBoardLogger):
                self.log_images_tensorboard(
                    logger=trainer.logger, 
                    inputs=outputs['input'], 
                    labels=batch['target'], 
                    preds_rgb=outputs['rgb_output'], 
                    preds_alpha=outputs['alpha_output'],
                    preds_reconstructed=outputs['reconstructed'],
                    flow=flow_image,
                    stage='val', 
                    global_step=trainer.global_step
                )
            elif isinstance(trainer.logger, WandbLogger):
                self.log_images_wandb(
                    logger=trainer.logger, 
                    inputs=outputs['input'], 
                    labels=batch['target'] if "target" in batch else None, 
                    preds_rgb=outputs['rgb_output'],
                    preds_alpha=outputs['alpha_output'],
                    preds_reconstructed=outputs['reconstructed'],
                    flow=flow_image,
                    stage='val', 
                    global_step=trainer.global_step
                )

    def log_images_tensorboard(self, logger, inputs, labels, preds_rgb, preds_alpha, preds_reconstructed, flow, stage, global_step):
        import torchvision.utils as vutils

        # Create a grid of input images
        grid_inputs = vutils.make_grid(inputs)
        logger.experiment.add_image(f'{stage}/inputs', grid_inputs, global_step)

        # Create a grid of label images (assuming labels are single-channel)
        grid_labels = vutils.make_grid(labels)
        logger.experiment.add_image(f'{stage}/labels', grid_labels, global_step)

        # Create a grid of prediction images (assuming preds are single-channel)
        grid_preds = vutils.make_grid(preds_rgb)
        logger.experiment.add_image(f'{stage}/predictions_rgb', grid_preds, global_step)

        # Create a grid of alpha images (assuming preds are single-channel)
        grid_alpha = vutils.make_grid(preds_alpha)
        logger.experiment.add_image(f'{stage}/predictions_alpha', grid_alpha, global_step)

        grid_reconstructed = vutils.make_grid(preds_reconstructed)
        logger.experiment.add_image(f'{stage}/reconstructed', grid_reconstructed, global_step)

        grid_flow = vutils.make_grid(flow)
        logger.experiment.add_image(f'{stage}/flow', grid_flow, global_step)

        

    def log_images_wandb(self, logger, inputs, labels, preds_rgb, preds_alpha, preds_reconstructed, flow, stage, global_step):
        import wandb
        import torchvision.utils as vutils

        # Create a grid of input images
        grid_inputs = vutils.make_grid(inputs)
        grid_inputs = grid_inputs.permute(1, 2, 0).cpu().float().numpy()  # Convert to HWC format
        wandb_inputs = wandb.Image(grid_inputs, caption=f'{stage}/inputs')

        # Create a grid of label images (assuming labels are single-channel)
        grid_labels = vutils.make_grid(labels)
        grid_labels = grid_labels.permute(1, 2, 0).cpu().float().numpy()  # Convert to HWC format
        wandb_labels = wandb.Image(grid_labels, caption=f'{stage}/labels')

        # Create a grid of prediction images (assuming preds are single-channel)
        grid_preds = vutils.make_grid(preds_rgb)
        grid_preds = grid_preds.permute(1, 2, 0).cpu().float().numpy()  # Convert to HWC format
        wandb_preds = wandb.Image(grid_preds, caption=f'{stage}/predictions')

        # Create a grid of alpha images (assuming preds are single-channel)
        grid_alpha = vutils.make_grid(preds_alpha)
        grid_alpha = grid_alpha.permute(1, 2, 0).cpu().float().numpy()
        wandb_alpha = wandb.Image(grid_alpha, caption=f'{stage}/alpha')

        grid_reconstructed = vutils.make_grid(preds_reconstructed)
        grid_reconstructed = grid_reconstructed.permute(1, 2, 0).cpu().float().numpy()
        wandb_reconstructed = wandb.Image(grid_reconstructed, caption=f'{stage}/reconstructed')

        grid_flow = vutils.make_grid(flow)
        grid_flow = grid_flow.permute(1, 2, 0).cpu().float().numpy()
        wandb_flow = wandb.Image(grid_flow, caption=f'{stage}/flow')

        # Log the images to wandb
        logger.experiment.log({
            f'{stage}/inputs': wandb_inputs,
            f'{stage}/labels': wandb_labels,
            f'{stage}/predictions_rgb': wandb_preds,
            f'{stage}/predictions_alpha': wandb_alpha,
            f'{stage}/reconstructed': wandb_reconstructed,
            f'{stage}/flow': wandb_flow,
            'global_step': global_step
        })

================================================================================

./video_dip/losses/flow_similarity_loss.py:

import torch
from torch import nn
from torchvision.models import vgg16
from video_dip.models.vgg import VGG
class FlowSimilarityLoss(nn.Module):
    """
    
    
    Attributes:
        
    """
    def __init__(self):
        super(FlowSimilarityLoss, self).__init__()
        

    def forward(self, m, frgb):
        """
        Calculate the flow similarity loss.

        Args:
            M (torch.Tensor): b,i(ith layer),h,w Motion tensor of all video layers at time t(mask)
            F^RGB (torch.Tensor): b,c,h,w RGB image flow from t-1 to t

        Returns:
            torch.Tensor: Flow Similarity Loss
        """
        (b,i,h,w) = m.size()
        (b,c,h,w) = frgb.size()

        

        m = m.repeat(1,c,1,1)
        frgb = frgb.repeat(1,i,1,1)
        
        # their product will be b,i,c,h,w we need to get rid of the i in order to feed to the VGG
        # therefore put that dimension into the batch as well
        production = frgb * m
        neg_production = frgb * (1-m)
        VGG.to(m.device)
        product = VGG(production.view(b*i, c,h,w))
        neg_product = VGG(neg_production.view(b*i, c,h,w))
        lfsim = (product * neg_product) / (torch.norm(product) * torch.norm(neg_product))
        return lfsim


================================================================================

./video_dip/losses/optical_flow_warp_loss.py:

import torch
from torch import nn
import torch.nn.functional as F

class OpticalFlowWarpLoss(nn.Module):
    """
    Optical flow warp loss module to ensure temporal coherence.
    """
    def __init__(self):
        super(OpticalFlowWarpLoss, self).__init__()
        self.l2_loss = nn.MSELoss()

    def forward(self, flow, prev_out, alpha_out):
        """
        Calculate the optical flow warp loss.

        Args:
            flow (torch.Tensor): Optical flow tensor.
            x (torch.Tensor): Ground truth tensor at time t-1.
            x_hat (torch.Tensor): Predicted tensor at time t.

        Returns:
            torch.Tensor: Optical flow warp loss.
        """
        warped = self.warp(prev_out, flow)
        return self.l2_loss(warped, alpha_out.repeat(1, 3, 1, 1))
    
    @staticmethod
    def warp(x, flow):
        """
        Warp an image/tensor (x) according to the given flow.

        Args:
            x (torch.Tensor): Image tensor of shape (N, C, H, W).
            flow (torch.Tensor): Optical flow tensor of shape (N, 2, H, W).

        Returns:
            torch.Tensor: Warped image tensor of shape (N, C, H, W).
        """
        N, C, H, W = x.size()
        # Create mesh grid
        grid_y, grid_x = torch.meshgrid(torch.arange(0, H), torch.arange(0, W))
        grid = torch.stack((grid_x, grid_y), 2).float()  # (H, W, 2)
        grid = grid.unsqueeze(0).repeat(N, 1, 1, 1).to(x.device)  # (N, H, W, 2)
        
        # Normalize the grid to [-1, 1]
        grid[:, :, :, 0] = 2.0 * grid[:, :, :, 0] / (W - 1) - 1.0
        grid[:, :, :, 1] = 2.0 * grid[:, :, :, 1] / (H - 1) - 1.0
        
        # Add optical flow to the grid
        flow = flow.permute(0, 2, 3, 1)  # (N, H, W, 2)
        vgrid = grid + flow
        
        # Normalize vgrid to [-1, 1]
        vgrid[:, :, :, 0] = 2.0 * vgrid[:, :, :, 0] / (W - 1) - 1.0
        vgrid[:, :, :, 1] = 2.0 * vgrid[:, :, :, 1] / (H - 1) - 1.0
        
        # Perform grid sampling
        output = F.grid_sample(x, vgrid, mode='bilinear', padding_mode='border', align_corners=True)
        return output

================================================================================

./video_dip/losses/__init__.py:

# import torch
from .reconstruction_loss import ReconstructionLoss
from .perceptual_loss import PerceptualLoss
from .optical_flow_warp_loss import OpticalFlowWarpLoss
from .flow_similarity_loss import FlowSimilarityLoss
from .mask_loss import MaskLoss
from .reconstruction_layer_loss import ReconstructionLayerLoss

# fismloss = FlowSimilarityLoss()
# i = 2
# b = 64
# # b*i --> 128
# masks = torch.randn(64,i,64,64)
# imgs = torch.randn(b,3,64,64)
# fsimloss_value = fismloss(masks, imgs)
# print(fsimloss_value.shape)

================================================================================

./video_dip/losses/reconstruction_loss.py:

import torch
from torch import nn

from video_dip.losses.perceptual_loss import PerceptualLoss


class ReconstructionLoss(nn.Module):
    """
    Reconstruction loss module combining L1 loss and perceptual loss.
    
    Attributes:
        perceptual_loss (PerceptualLoss): Perceptual loss module.
    """
    def __init__(self):
        super(ReconstructionLoss, self).__init__()
        self.perceptual_loss = PerceptualLoss()
        self.l1_loss = nn.L1Loss()

    def forward(self, x, x_hat):
        """
        Calculate the reconstruction loss.

        Args:
            x (torch.Tensor): Ground truth tensor.
            x_hat (torch.Tensor): Predicted tensor.

        Returns:
            torch.Tensor: Combined reconstruction loss.
        """
        # l1_loss = torch.mean(torch.abs(x - x_hat))  # L1 loss
        l1_loss = self.l1_loss(x, x_hat)
        perceptual_loss = self.perceptual_loss(x, x_hat)  # Perceptual loss
        return l1_loss + perceptual_loss


================================================================================

./video_dip/losses/reconstruction_layer_loss.py:

import torch
from torch import nn

from video_dip.losses.perceptual_loss import PerceptualLoss


class ReconstructionLayerLoss(nn.Module):
    """
    Reconstruction loss module combining L1 loss and perceptual loss.
    
    Attributes:
        l1_loss (L1Loss): L1 loss module.
    """
    def __init__(self):
        super(ReconstructionLayerLoss, self).__init__()
        self.l1_loss = nn.L1Loss()

    def forward(self, m, x, x_hat):
        """
        Calculate the reconstruction loss.

        Args:
            m (torch.Tensor) : alpha net output
            x (torch.Tensor): Ground truth tensor.
            x_hat (torch.Tensor): Predicted tensor.

        Returns:
            torch.Tensor: Combined reconstruction loss.
        """

        (b,i,h,w) = m.size()
        (b,c,h,w) = x.size()
        
        
        second_dim = i*c

        m = m.repeat(1,c,1,1)
        x = x.repeat(1,i,1,1)
        x_hat = x_hat.repeat(1,i,1,1)

        # their product will be b,i,c,h,w we need to get rid of the i in order to feed to the VGG
        # therefore put that dimension into the batch as well
        production = x * m
        pred_production = x_hat * m

        layer_loss = self.l1_loss(production, pred_production)
        
        return layer_loss


================================================================================

./video_dip/losses/mask_loss.py:

import torch
from torch import nn
from torchvision.models import vgg16
from video_dip.models.vgg import VGG
class MaskLoss(nn.Module):
    """
    
    
    Attributes:
        
    """
    def __init__(self):
        super(MaskLoss, self).__init__()
        

    def forward(self, m):
        """
        Calculate the flow similarity loss.

        Args:
            M (torch.Tensor): b,i(ith layer),h,w Motion tensor of all video layers at time t(mask)


        Returns:
            torch.Tensor: Mask Loss
        """
        (b,i,h,w) = m.size()
        return torch.sum(torch.abs(m - 0.5) ** -1, dim = (0,1))



================================================================================

./video_dip/losses/perceptual_loss.py:

import torch
from torch import nn
# from torchvision.models import vgg16
from video_dip.models import VGG

class PerceptualLoss(nn.Module):
    """
    Perceptual loss module using a pretrained VGG16 network.
    
    Attributes:
        vgg (nn.Module): Pretrained VGG16 network truncated at the 16th layer.
    """
    def __init__(self):
        super(PerceptualLoss, self).__init__()
        self.l1_loss = nn.L1Loss()

    def forward(self, x, y):
        """
        Calculate the perceptual loss between two tensors.

        Args:
            x (torch.Tensor): Predicted tensor.
            y (torch.Tensor): Ground truth tensor.

        Returns:
            torch.Tensor: Perceptual loss.
        """
        try:
            x_features = VGG(x)
        except RuntimeError:
            VGG.to(x.device)
            x_features = VGG(x)
        y_features = VGG(y)
        return self.l1_loss(x_features, y_features)


================================================================================

./video_dip/models/vgg.py:

from torchvision.models import vgg16

VGG = vgg16(pretrained=True).features[:16].eval()
for param in VGG.parameters():
    param.requires_grad = False

================================================================================

./video_dip/models/__init__.py:

from .unet import UNet
from .vgg import VGG

================================================================================

./video_dip/models/unet.py:

import torch.nn as nn

class UNet(nn.Module):
    """
    UNet is a convolutional neural network architecture for image segmentation.
    It consists of an encoder and a decoder, with skip connections between them.

    Args:
        in_channels (int): Number of input channels (default: 3)
        channels (list): List of channel sizes for each layer in the encoder and decoder (default: [64, 64, 96, 128, 128, 128, 128, 96])

    Attributes:
        encoder (nn.Sequential): Encoder module consisting of convolutional layers and max pooling layers
        decoder (nn.Sequential): Decoder module consisting of up-convolutional layers and upsampling layers

    """

    def __init__(self, out_channels, in_channels=3, channels=[64, 64, 96, 128, 128, 128, 128, 96]):
        super(UNet, self).__init__()
        self.encoder = nn.Sequential(
            self._conv(in_channels, channels[0]),
            self._conv(channels[0], channels[1]),
            nn.MaxPool2d(2),
            self._conv(channels[1], channels[2]),
            self._conv(channels[2], channels[3]),
            nn.MaxPool2d(2),
            self._conv(channels[3], channels[4]),
            self._conv(channels[4], channels[5]),
            self._conv(channels[5], channels[6]),
            nn.MaxPool2d(2),
            nn.Sequential(
                nn.Conv2d(channels[6], channels[7], 4, 1, 0),
                nn.Sigmoid()
            )
        )
        self.decoder = nn.Sequential(
            self._upconv(channels[7], channels[6], 4, 1, 0),
            nn.Upsample(scale_factor=2, mode='bicubic'),
            self._conv(channels[6], channels[5]),
            self._conv(channels[5], channels[4]),
            self._conv(channels[4], channels[3]),
            nn.Upsample(scale_factor=2, mode='bicubic'),
            self._conv(channels[3], channels[2]),
            self._conv(channels[2], channels[1]),
            nn.Upsample(scale_factor=2, mode='bicubic'),
            self._conv(channels[1], channels[0]),
            nn.Sequential(
                nn.ConvTranspose2d(channels[0], out_channels, 3, 1, 1),
                nn.Sigmoid()
            )
        )

    def _conv(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        """
        Helper function to create a convolutional layer with batch normalization and LeakyReLU activation.

        Args:
            in_channels (int): Number of input channels
            out_channels (int): Number of output channels
            kernel_size (int): Size of the convolutional kernel (default: 3)
            stride (int): Stride of the convolution (default: 1)
            padding (int): Padding of the convolution (default: 1)

        Returns:
            nn.Sequential: Convolutional layer with batch normalization and LeakyReLU activation
        """
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2)
        )

    def _upconv(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):
        """
        Helper function to create an up-convolutional layer with batch normalization and LeakyReLU activation.

        Args:
            in_channels (int): Number of input channels
            out_channels (int): Number of output channels
            kernel_size (int): Size of the up-convolutional kernel (default: 3)
            stride (int): Stride of the up-convolution (default: 1)
            padding (int): Padding of the up-convolution (default: 1)

        Returns:
            nn.Sequential: Up-convolutional layer with batch normalization and LeakyReLU activation
        """
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding),
            nn.BatchNorm2d(out_channels),
            nn.LeakyReLU(0.2)
        )

    def forward(self, x):
        """
        Forward pass of the UNet model.

        Args:
            x (torch.Tensor): Input tensor

        Returns:
            torch.Tensor: Output tensor
        """
        encoder_outputs = []
        for layer in self.encoder:
            x = layer(x)
            encoder_outputs.append(x)
        
        for idx, layer in enumerate(self.decoder):
            x = layer(x + encoder_outputs[-(idx + 1)])
        
        return x

================================================================================

./video_dip/models/optical_flow/__init__.py:

from .farneback import Farneback
from .raft import RAFT
from .raft import RAFTModelSize

================================================================================

./video_dip/models/optical_flow/raft.py:

import torch
import torchvision
from typing import Any, Tuple, Union, List
from enum import Enum
from torchvision.utils import flow_to_image
from torch.cuda.amp import autocast

class RAFTModelSize(Enum):
    SMALL = 'raft_small'
    LARGE = 'raft_large'

class RAFT:
    def __init__(self, model_size: RAFTModelSize = RAFTModelSize.SMALL, **kwargs: Any):
        """
        Initializes the RAFTWrapper with the specified model size.

        Args:
            model_size (RAFTModelSize): The size of the RAFT model to use.
            kwargs (Any): Additional arguments for the model.
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model, self.raft_transforms = self._load_model(model_size, **kwargs)
        self.model.eval()
        self._original_size = None

    def half(self):
        """
        Converts the model to half precision.

        Returns:
            RAFT: The model in half precision.
        """
        self.model = self.model.half()
        return self

    def _load_model(self, model_size: RAFTModelSize, **kwargs: Any) -> torch.nn.Module:
        """
        Loads the specified model.

        Args:
            model_size (RAFTModelSize): The size of the model to use.
            kwargs (Any): Additional arguments for the model.

        Returns:
            torch.nn.Module: The loaded model.
        """
        if model_size == RAFTModelSize.SMALL:
            weights = torchvision.models.optical_flow.Raft_Small_Weights.DEFAULT
            model = torchvision.models.optical_flow.raft_small(weights, **kwargs)
            transforms = weights.transforms()
        elif model_size == RAFTModelSize.LARGE:
            weights = torchvision.models.optical_flow.Raft_Large_Weights.DEFAULT
            model = torchvision.models.optical_flow.raft_large(weights, **kwargs)
            transforms = weights.transforms()
        else:
            raise ValueError(f"Model size {model_size} is not supported.")
        return model.to(self.device), transforms

    def forward(self, image1: Union[torch.Tensor, List[Any], Any], image2: Union[torch.Tensor, List[Any], Any]) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Runs a forward pass through the model.

        Args:
            image1 (Union[torch.Tensor, List[Any], Any]): The first image tensor, list of images, or single image.
            image2 (Union[torch.Tensor, List[Any], Any]): The second image tensor, list of images, or single image.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]: The flow predictions.
        """
        image1 = self._prepare_input(image1)
        image2 = self._prepare_input(image2)

        image1, image2 = self.raft_transforms(image1, image2)

        image1, image2 = image1.to(self.device), image2.to(self.device)

        with torch.no_grad():
            with autocast():
                flow_predictions = self.model(image1, image2)
        return self.postprocess(flow_predictions)
    
    def __call__(self, *args: Any, **kwds: Any) -> Any:
        """
        Calls the forward method of the model.

        Args:
            *args (Any): The arguments to pass to the forward method.
            **kwds (Any): The keyword arguments to pass to the forward method.

        Returns:
            Any: The output of the forward method.
        """
        return self.forward(*args, **kwds)

    def _prepare_input(self, image: Union[torch.Tensor, List[Any], Any]) -> torch.Tensor:
        """
        Prepares the input images for processing.

        Args:
            image (Union[torch.Tensor, List[Any], Any]): The input image(s).

        Returns:
            torch.Tensor: The prepared image tensor.
        """
        if isinstance(image, list):
            self._original_size = image[0].shape
            if not all(isinstance(img, torch.Tensor) for img in image):
                image = [self._apply_transform(img) for img in image]
            image = torch.stack(image)
        elif not isinstance(image, torch.Tensor):
            self._original_size = image.size
            image = self._apply_transform(image)
            image = image.unsqueeze(0)
        elif len(image.shape) == 3:
            self._original_size = image.shape
            image = image.unsqueeze(0)
            image = self._apply_transform(image)
        elif len(image.shape) == 4:
            self._original_size = image[0].shape
            image = self._apply_transform(image)
        return image

    def _apply_transform(self, image: Any) -> torch.Tensor:
        """
        Applies the transformation to the input image.

        Args:
            image (Any): The input image.

        Returns:
            torch.Tensor: The transformed image.
        """
        transform = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Resize((520, 960), antialias=False),
        ])
        if isinstance(image, torch.Tensor):
            transform.transforms = transform.transforms[1:]
        return transform(image)

    def postprocess(self, flow: torch.Tensor) -> Any:
        """
        Postprocesses the flow output.

        Args:
            flow (torch.Tensor): The flow output.

        Returns:
            Any: The postprocessed flow.
        """
        # Return np array
        return flow[-1].cpu().numpy().squeeze(0)
# Usage example
if __name__ == "__main__":
    from PIL import Image
    import numpy as np

    # Create example single image
    single_image1 = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))
    single_image2 = Image.fromarray((np.random.rand(256, 256, 3) * 255).astype(np.uint8))

    # Create example batched images
    batched_image1 = np.random.rand(2, 3, 256, 256).astype(np.float32)
    batched_image2 = np.random.rand(2, 3, 256, 256).astype(np.float32)
    
    batched_image1 = torch.tensor(batched_image1)
    batched_image2 = torch.tensor(batched_image2)

    # Initialize RAFT wrapper with model size choice
    raft = RAFT(model_size=RAFTModelSize.SMALL)

    # Get flow predictions for single images
    flow_predictions = raft(single_image1, single_image2)

    # Postprocess and display flow for single images
    flow = raft.postprocess(flow_predictions)
    print(flow)

    # Get flow predictions for batched images
    flow_predictions = raft(batched_image1, batched_image2)

    # Postprocess and display flow for batched images
    flow = raft.postprocess(flow_predictions)
    print(flow)


================================================================================

./video_dip/models/optical_flow/farneback.py:

import cv2
import numpy as np
import torch
from torch import nn

class Farneback:
    """
    Optical flow estimation using Lucas-Kanade method from OpenCV.
    """
    def __init__(self):
        pass

    def _farneback_one_image(self, image1, image2):
        """
        Estimate the optical flow between two images.

        Args:
            image1 (np.ndarray): First image (grayscale).
            image2 (np.ndarray): Second image (grayscale).

        Returns:
            np.ndarray: Estimated optical flow.
        """
        # Permute dimensions if necessary
        if (len(image1.shape) == 3) and (image1.shape[0] == 3):
            image1 = np.transpose(image1, (1, 2, 0))
            image2 = np.transpose(image2, (1, 2, 0))

        if (len(image1.shape) == 3) and (image1.shape[2] == 3):
            image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)
            image2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)

        # Calculate optical flow
        flow = cv2.calcOpticalFlowFarneback(image1, image2, None, 0.5, 3, 15, 3, 5, 1.2, 0)
        # # Compute magnite and angle of 2D vector
        # mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])

        # hsv_mask = np.zeros((image1.shape[0], image1.shape[1], 3), dtype=np.uint8)
        # hsv_mask[..., 1] = 255

        # # Set image hue value according to the angle of optical flow
        # hsv_mask[..., 0] = ang * 180 / np.pi / 2
        # # Set value as per the normalized magnitude of optical flow
        # hsv_mask[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)
        # # Convert to rgb
        # rgb_representation = cv2.cvtColor(hsv_mask, cv2.COLOR_HSV2BGR)
        # Convert flow to tensor
        flow_rgb = torch.tensor(flow).permute(2, 0, 1).unsqueeze(0).float()
        return flow_rgb
    
    def forward(self, image1, image2):
        """
        Estimate the optical flow between two images.

        Args:
            image1 (torch.Tensor): First image (grayscale).
            image2 (torch.Tensor): Second image (grayscale).

        Returns:
            torch.Tensor: Estimated optical flow.
        """
        image1 = image1.squeeze().cpu().numpy()
        image2 = image2.squeeze().cpu().numpy()

        # Convert images to uint8
        image1 = np.uint8(image1 * 255)
        image2 = np.uint8(image2 * 255)

        ret = None
        # Convert images to grayscale if they are RGB
        if len(image1.shape) == 4:
            flows = []
            for i in range(len(image1)):
                flows.append(self._farneback_one_image(image1[i], image2[i]))
            ret = torch.cat(flows)
        else:
            ret = self._farneback_one_image(image1, image2)

        return self.postprocess(ret)
        
    def postprocess(self, flow):
        """
        Postprocess the estimated optical flow.

        Args:
            flow (torch.Tensor): Estimated optical flow.

        Returns:
            np.ndarray: Postprocessed optical flow.
        """
        return flow.squeeze().numpy()
        
    def forward_clip(self, clip):
        """
        Estimate the optical flow between frames in a video clip.

        Args:
            clip (torch.Tensor): Video clip with frames (grayscale).

        Returns:
            torch.Tensor: Estimated optical flow.
        """
        clip = clip.squeeze().cpu().numpy()
        clip = np.uint8(clip * 255)

        flows = []
        for i in range(len(clip) - 1):
            flows.append(self._farneback_one_image(clip[i], clip[i + 1]))
        return torch.cat(flows)
        
    def __call__(self, inp, inp2 = None):
        if inp2 is not None:
            return self.forward(inp, inp2)
        else:
            return self.forward_clip(inp)

if __name__ == '__main__':
    import sys
    import os
    # Add parent of parent directory to path
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), os.pardir, os.pardir)))

    from video_dip.data import VideoDIPDataset
    from torch.utils.data import DataLoader

    dataset = VideoDIPDataset("video_dip/data/sora.mp4")
    data_loader = DataLoader(dataset, batch_size=2, num_workers=8)

    lucas_kanade = Farneback()
    batch = next(iter(data_loader))
    flow = lucas_kanade(batch['input'])

    print(flow.shape)

    import PIL
    PIL.Image.fromarray((flow.squeeze().permute(1, 2, 0).numpy()).astype('uint8')).show()


================================================================================

./video_dip/models/modules/dehaze.py:

import torch
from . import VDPModule
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure

class DehazeVDPModule(VDPModule):
    """
    A module for dehazing in VideoDIP.

    Args:
        learning_rate (float): The learning rate for optimization (default: 1e-3).
        loss_weights (list): The weights for different losses (default: [1, .02]).
    """

    def __init__(self, learning_rate=1e-3, loss_weights=[1, .02], **kwargs):
        super().__init__(learning_rate, loss_weights, **kwargs)

        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)
        self.psnr = PeakSignalNoiseRatio(data_range=1.0)

        self.save_hyperparameters()

    def reconstruction_fn(self, rgb_output, alpha_output, airlight):
        """
        Reconstructs the output by performing element-wise multiplication of RGB layers with alpha layers.

        Args:
            rgb_output (torch.Tensor): The RGB output tensor.
            alpha_output (torch.Tensor): The alpha output tensor.

        Returns:
            torch.Tensor: The reconstructed output tensor.
        """

        return (alpha_output * rgb_output) + ((1 - alpha_output) * airlight.view(airlight.size(0), airlight.size(1), 1, 1))
    
    def training_step(self, batch, batch_idx):
        outputs = self.inference(batch, batch_idx, airlight=batch['airlight'])

        prev_output = self(img=batch['prev_input'])['rgb'].detach()

        rec_loss = self.reconstruction_loss(outputs['input'], outputs['reconstructed'])
        warp_loss = self.warp_loss(outputs['flow'], prev_output, outputs['alpha_output'])

        loss = self.loss_weights[0] * rec_loss + self.loss_weights[1] * warp_loss

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=4)
        self.log('rec_loss', rec_loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=4)
        self.log('warp_loss', warp_loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=4)

        return loss
    
    def validation_step(self, batch, batch_idx):
        outputs = self.inference(batch, batch_idx, airlight=batch['airlight'])
        
        rgb_output = outputs['rgb_output']
        gt = batch['target']

        # Compute PSNR and SSIM
        self.psnr(rgb_output, gt)
        self.ssim(rgb_output, gt)

        self.log('psnr', self.psnr, on_step=False, on_epoch=True, prog_bar=True, batch_size=4)
        self.log('ssim', self.ssim, on_step=False, on_epoch=True, prog_bar=True, batch_size=4)

        return outputs
    

================================================================================

./video_dip/models/modules/__init__.py:

from .base import VDPModule
from .relight import RelightVDPModule
from .dehaze import DehazeVDPModule

================================================================================

./video_dip/models/modules/segmentation.py:

import torch
import torchvision
from . import VDPModule
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure
from video_dip.losses import FlowSimilarityLoss, MaskLoss, ReconstructionLayerLoss
from video_dip.models.unet import UNet


class SegmentationVDPModule(VDPModule):
    """
    A module for segmentation in VideoDIP.

    Args:
        learning_rate (float): The learning rate for optimization (default: 1e-3).
        loss_weights (list): The weights for different losses (default: [1, .02]).
            flow similarity weight
            reconstruction loss weight
            layer loss weight
            warp loss weight
            mask mask loss weight
    """
        #rec = 1, Fsim = 0.001, layer = 1,
        #warp = 0.01 and Mask = 0.01.


    def __init__(self, learning_rate=1e-3, loss_weights=[.001, 1, 1, .001, .01]):
        super().__init__(learning_rate, loss_weights)


        # one additional rgb net
        self.rgb_net2 = UNet(out_channels=3)  # RGB-Net with 3 input and 3 output channels

        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)
        self.psnr = PeakSignalNoiseRatio(data_range=1.0)


        self.flow_similarity_loss = FlowSimilarityLoss()
        self.rec_layer_loss = ReconstructionLayerLoss()
        self.mask_loss = MaskLoss()

        self.save_hyperparameters()

    def reconstruction_fn(self, rgb_output1, rgb_output2, alpha_output):
        """
        Reconstructs the output by performing element-wise multiplication of RGB layers with alpha layers.

        Args:
            rgb_output1 (torch.Tensor): The RGB output tensor.
            rgb_output2 (torch.Tensor): The RGB output tensor.
            alpha_output (torch.Tensor) b,i,h,w : The alpha output tensor.

        Returns:
            torch.Tensor: The reconstructed output tensor.
        """
        # rescale Mi's so that after addition of all masks they sum up to one (falpha)
        summed_masks = torch.sum(alpha_output, dim = 1)
        summed_masks = torch.unsqueeze(summed_masks, 1)

        alpha_output = alpha_output / summed_masks


        return alpha_output * rgb_output1 + (1 - alpha_output) * rgb_output2
    
    def forward(self, img=None, flow=None):
        """
        Performs forward pass through the network.

        Args:
            img (torch.Tensor): The input image tensor. Default is None.
            flow (torch.Tensor): The input optical flow tensor. Default is None.

        Returns:
            dict: A dictionary containing the output tensors.

        """
        ret = {}
        if img is not None:
            ret['rgb'] = self.rgb_net(img)
            ret["rgb2"] = self.rgb_net2(img)
        if flow is not None:
            ret['alpha'] = self.alpha_net(flow)
        return ret

    def inference(self, batch, batch_idx):
        input_frames = batch['input']
        flows = batch['flow']

        flow_frames = torchvision.utils.flow_to_image(flows) / 255.0

        output = self(img=input_frames, flow=flow_frames)
        rgb_output = output['rgb']
        rgb_output2 = output["rgb2"]
        alpha_output = output['alpha']

        reconstructed_frame = self.reconstruction_fn(rgb_output, rgb_output2 ,alpha_output)

        return {
            "input": input_frames,
            "flow": flows,
            "reconstructed": reconstructed_frame,
            "rgb_output": rgb_output,
            "rgb_output2": rgb_output2,
            "alpha_output": alpha_output
        }
    
    

    def training_step(self, batch, batch_idx):
        outputs = self.inference(batch, batch_idx)
        flow_estimate = torchvision.utils.flow_to_image(batch['prev_flow']) / 255.0
        prev_alpha_output = self(flow=flow_estimate)['alpha']

        x = batch["input"]
        x_hat = outputs["reconstructed"]



        flow_sim_loss = self.flow_similarity_loss(m = prev_alpha_output, frgb = flow_estimate)
        rec_loss = self.reconstruction_loss(x = x, x_hat = x_hat)
        rec_layer_loss = self.rec_layer_loss(m = flow_estimate, x = x, x_hat = x_hat)
        warp_loss = self.warp_loss(outputs['flow'], prev_alpha_output, outputs['alpha_output'])
        mask_loss = self.mask_loss(outputs['alpha_output'])


        #     flow similarity weight
        #     reconstruction loss weight
        #     layer loss weight
        #     warp loss weight
        #     mask mask loss weight



        flow_sim_loss =self.loss_weights[0] * torch.mean( flow_sim_loss)
        rec_loss = self.loss_weights[1] * torch.mean( rec_loss)
        rec_layer_loss = self.loss_weights[2] * torch.mean( rec_layer_loss)
        warp_loss = self.loss_weights[3] * torch.mean( warp_loss)
        mask_loss = self.loss_weights[4] * torch.mean( mask_loss)


        loss = flow_sim_loss
        + rec_loss
        + rec_layer_loss
        + warp_loss
        + mask_loss
        



        self.log("train_loss", loss, prog_bar=True, on_step=False, on_epoch=True)
        self.log("rec_loss", rec_loss, prog_bar=False, on_step=False, on_epoch=True)
        self.log("warp_loss", warp_loss, prog_bar=False, on_step=False, on_epoch=True)
        self.log("flow_sim_loss", flow_sim_loss, prog_bar=False, on_step=False, on_epoch=True)
        self.log("rec_layer_loss", rec_layer_loss, prog_bar=False, on_step=False, on_epoch=True)
        self.log("mask_loss", mask_loss, prog_bar=False, on_step=False, on_epoch=True)
        

        return loss
    

    
    def validation_step(self, batch, batch_idx):
        outputs = self.inference(batch, batch_idx)

        # rgb_output = outputs['rgb_output']
        # gt = batch['target']

        # # Compute PSNR and SSIM
        # self.psnr(rgb_output, gt)
        # self.ssim(rgb_output, gt)

        self.log('a', 1, on_step=False, on_epoch=True, prog_bar=True)
        # self.log('ssim', self.ssim, on_step=False, on_epoch=True, prog_bar=True)
        # self.log('gamma_inv', self.gamma_inv, on_step=False, on_epoch=True, prog_bar=True)

        return outputs
    


================================================================================

./video_dip/models/modules/relight.py:

import torch
from . import VDPModule
from torchmetrics import PeakSignalNoiseRatio, StructuralSimilarityIndexMeasure

class RelightVDPModule(VDPModule):
    """
    A module for relighting in VideoDIP.

    Args:
        learning_rate (float): The learning rate for optimization (default: 1e-3).
        loss_weights (list): The weights for different losses (default: [1, .02]).
    """

    def __init__(self, learning_rate=5e-5, loss_weights=[1, .02], **kwargs):
        super().__init__(learning_rate, loss_weights, **kwargs)

        # Randomly initialize a parameter named gamma
        self.gamma_inv = torch.nn.Parameter(torch.tensor(.9))

        self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0)
        self.psnr = PeakSignalNoiseRatio(data_range=1.0)

        self.save_hyperparameters()

    def reconstruction_fn(self, rgb_output, alpha_output, **kwargs):
        """
        Reconstructs the output by performing element-wise multiplication of RGB layers with alpha layers.

        Args:
            rgb_output (torch.Tensor): The RGB output tensor.
            alpha_output (torch.Tensor): The alpha output tensor.

        Returns:
            torch.Tensor: The reconstructed output tensor.
        """
        if 'reconstruct' in kwargs and kwargs['reconstruct']=='logarithmic':
            return self.gamma_inv * (torch.log(alpha_output) + torch.log(rgb_output))
        return alpha_output * rgb_output ** self.gamma_inv
    
    def training_step(self, batch, batch_idx):
        outputs = self.inference(batch, batch_idx)#, reconstruct='logarithmic')

        prev_output = self(img=batch['prev_input'])['rgb'].detach()

        # rec_loss = self.reconstruction_loss(x_hat=outputs['reconstructed'], x=torch.log(batch['input'] + 1e-9))
        rec_loss = self.reconstruction_loss(x_hat=outputs['reconstructed'], x=batch['input'])
        warp_loss = self.warp_loss(
            flow=outputs['flow'], 
            prev_out=prev_output, 
            alpha_out=outputs['alpha_output']
        )

        loss = self.loss_weights[0] * rec_loss + self.loss_weights[1] * warp_loss

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, batch_size=4)
        self.log('rec_loss', rec_loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=4)
        self.log('warp_loss', warp_loss, on_step=False, on_epoch=True, prog_bar=False, batch_size=4)

        return loss
    
    def validation_step(self, batch, batch_idx):
        outputs = super().validation_step(batch, batch_idx)

        rgb_output = outputs['rgb_output']
        gt = batch['target']

        # Compute PSNR and SSIM
        self.psnr(rgb_output, gt)
        self.ssim(rgb_output, gt)

        self.log('psnr', self.psnr, on_step=False, on_epoch=True, prog_bar=True, batch_size=4)
        self.log('ssim', self.ssim, on_step=False, on_epoch=True, prog_bar=True, batch_size=4)
        self.log('gamma_inv', self.gamma_inv, on_step=False, on_epoch=True, prog_bar=True, batch_size=4)

        return outputs
    

================================================================================

./video_dip/models/modules/base.py:

import pytorch_lightning as pl
import torch
import torchvision
from torch.optim.lr_scheduler import CosineAnnealingLR

from video_dip.models.unet import UNet
from video_dip.losses import ReconstructionLoss, OpticalFlowWarpLoss

class VDPModule(pl.LightningModule):
    """
    VDPModule is a PyTorch Lightning module for video deep image prior (VDIP) models.

    Args:
        learning_rate (float): The learning rate for the optimizer. Default is 1e-3.
        loss_weights (list): The weights for the reconstruction and warp losses. Default is [1, .02].

    Attributes:
        rgb_net (UNet): The UNet model for RGB inputs.
        alpha_net (UNet): The UNet model for optical flow inputs.
        learning_rate (float): The learning rate for the optimizer.
        reconstruction_loss (ReconstructionLoss): The reconstruction loss function.
        warp_loss (OpticalFlowWarpLoss): The optical flow warp loss function.
        loss_weights (list): The weights for the reconstruction and warp losses.

    Methods:
        forward(img=None, flow=None): Performs forward pass through the network.
        reconstruction_fn(rgb_output, alpha_output): Computes the reconstructed frame.
        inference(batch, batch_idx): Performs inference on a batch of data.
        training_step(batch, batch_idx): Defines the training step.
        validation_step(batch, batch_idx): Defines the validation step.
        configure_optimizers(): Configures the optimizer.

    """

    def __init__(self, learning_rate=1e-3, loss_weights=[1, .02], multi_step_scheduling_kwargs=None, warmup=False):
        super().__init__()
        self.rgb_net = UNet(out_channels=3)  # RGB-Net with 3 input and 3 output channels
        self.alpha_net = UNet(out_channels=1)  # Alpha-Net with 3 input and 1 output channels (for optical flow
        self.learning_rate = learning_rate

        self.reconstruction_loss = ReconstructionLoss()
        self.warp_loss = OpticalFlowWarpLoss()

        self.loss_weights = loss_weights

        self.warmup = warmup

        self.multi_step_scheduling_kwargs = multi_step_scheduling_kwargs

    def forward(self, img=None, flow=None):
        """
        Performs forward pass through the network.

        Args:
            img (torch.Tensor): The input image tensor. Default is None.
            flow (torch.Tensor): The input optical flow tensor. Default is None.

        Returns:
            dict: A dictionary containing the output tensors.

        """
        ret = {}
        if img is not None:
            ret['rgb'] = self.rgb_net(img)
        if flow is not None:
            ret['alpha'] = self.alpha_net(flow)
        return ret
        
    def reconstruction_fn(self, rgb_output, alpha_output, **kwargs):
        """
        Computes the reconstructed frame.

        Args:
            rgb_output (torch.Tensor): The RGB output tensor.
            alpha_output (torch.Tensor): The alpha output tensor.

        Raises:
            NotImplementedError: If the reconstruction function is not implemented.

        """
        raise NotImplementedError("The reconstruction function is not implemented.")

    def inference(self, batch, batch_idx, **kwargs):
        """
        Performs inference on a batch of data.

        Args:
            batch (dict): A dictionary containing the input and flow tensors.
            batch_idx (int): The index of the current batch.

        Returns:
            dict: A dictionary containing the input, flow, reconstructed, rgb_output, and alpha_output tensors.

        """
        input_frames = batch['input']
        flows = batch['flow']

        flow_frames = torchvision.utils.flow_to_image(flows) / 255.0

        output = self(img=input_frames, flow=flow_frames)
        rgb_output = output['rgb']
        alpha_output = output['alpha']

        reconstructed_frame = self.reconstruction_fn(rgb_output, alpha_output, **kwargs)

        return {
            "input": input_frames,
            "flow": flows,
            "reconstructed": reconstructed_frame,
            "rgb_output": rgb_output,
            "alpha_output": alpha_output
        }

    def training_step(self, batch, batch_idx):
        """
        Defines the training step.

        Args:
            batch (dict): A dictionary containing the input and flow tensors.
            batch_idx (int): The index of the current batch.

        Returns:
            torch.Tensor: The computed loss value.

        """
        outputs = self.inference(batch, batch_idx)
        # prev_alpha_output = self(flow=torchvision.utils.flow_to_image(batch['prev_input']) / 255.0)['alpha'].detach()
        prev_output = self(img=batch['prev_input'])['rgb'].detach()

        rec_loss = self.reconstruction_loss(outputs['input'], outputs['reconstructed'])
        warp_loss = self.warp_loss(outputs['flow'], prev_output, outputs['alpha_output'])

        loss = self.loss_weights[0] * rec_loss + self.loss_weights[1] * warp_loss

        self.log("train_loss", loss, prog_bar=True, on_step=False, on_epoch=True)
        self.log("rec_loss", rec_loss, prog_bar=False, on_step=False, on_epoch=True)
        self.log("warp_loss", warp_loss, prog_bar=False, on_step=False, on_epoch=True)

        return loss
    
    def validation_step(self, batch, batch_idx):
        """
        Defines the validation step.

        Args:
            batch (dict): A dictionary containing the input and flow tensors.
            batch_idx (int): The index of the current batch.

        """
        outputs = self.inference(batch, batch_idx)

        return outputs
        
    def configure_optimizers(self):
        """
        Configures the optimizer and scheduler.

        Returns:
            dict: A dictionary containing the optimizer and the LR scheduler.

        """
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        # MultiStep Scheduler
        ret = {'optimizer': optimizer}
        
        schedulers = []
        if self.warmup:
            # Warmup for 5 epochs from 2e-5 to self.learning_rate
            def lr_lambda(epoch):
                if epoch < 5:
                    warmup_lr = 2e-5 + epoch * (self.learning_rate - 2e-5) / 5
                    return warmup_lr / self.learning_rate
                return 1.0
            
            schedulers.append(torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda))
            ret['lr_scheduler'] = {
                'scheduler': schedulers[0],
                'interval': 'epoch',
                'frequency': 1,
            }

        if self.multi_step_scheduling_kwargs is not None:
            schedulers.append(torch.optim.lr_scheduler.MultiStepLR(optimizer, **self.multi_step_scheduling_kwargs))
            ret['lr_scheduler'] = {
                'scheduler': schedulers[0],
                'interval': 'epoch',
                'frequency': 1,
            }

        if len(schedulers) > 1:
            combined_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers, milestones=[5])
            ret['lr_scheduler'] = {
                'scheduler': combined_scheduler,
                'interval': 'epoch',
                'frequency': 1,
            }
        
        return ret
            
    def test_step(self, batch, batch_idx):        
        return self.validation_step(batch, batch_idx)

================================================================================

./datasets/relight/cleanup.py:

import os
import shutil

# Specify the path to the parent folder
parent_folder = '/home/furkan/projects/school/796/VideoDIP/datasets/outdoor_png/input'

# List of folders to keep
folders_to_keep = [
    'pair1', 'pair5', 'pair14', 'pair36', 'pair46', 'pair48', 
    'pair49', 'pair60', 'pair62', 'pair63', 'pair66', 'pair75', 'pair76'
]

# Get a list of all items in the parent folder
all_items = os.listdir(parent_folder)
all_items.sort()

# Loop through each item
for item in all_items:
    item_path = os.path.join(parent_folder, item)
    
    # Check if the item is a directory and not in the keep list
    if os.path.isdir(item_path) and item not in folders_to_keep:
        # Remove the directory
        shutil.rmtree(item_path)
        print(f'Deleted folder: {item}')

print('Cleanup complete.')


================================================================================

