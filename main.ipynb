{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIDEO DECOMPOSITION PRIOR: EDITING VIDEOS LAYER BY LAYER\n",
    "\n",
    "Paper Authors: \n",
    "\n",
    "| Name | Shcool | Mail |\n",
    "| ---- | ------ | ---- |\n",
    "| Gaurav Shrivastava | University of Maryland, College Park | gauravsh@umd.edu|\n",
    "| Abhinav Shrivastava | University of Maryland, College Park | abhinav@cs.umd.edu\n",
    "| Ser-Nam Lim | University of Central Florida | sernam@ucf.edu |\n",
    "\n",
    "\n",
    "Project Authors: Alper Bahçekapılı, Furkan Küçük\n",
    "\n",
    "\n",
    "Paper Summary: Paper is a deep learning framework to edit videos without supervision. Namely following three points are adressed in the paper:\n",
    "\n",
    "* Video Relighting\n",
    "* Video Dehazing\n",
    "* Unsupervised Video Object Segmentation\n",
    "\n",
    "\n",
    "\n",
    "## Overall Logic of the Paper:\n",
    "\n",
    "Paper approaches the problem with the intuition from video editing programs. As in these programs, they treat the videos as they are composed of multiple layers. For relighting problem, one layer is relight-map and the other is dark frame. In dehazing, again similar to reliht one layer is t-map etc. For segmentation, layers are foreground objects layer and the other is background layer. \n",
    "\n",
    "All optimization is done in the inference time. So for each of the video, we train models from the ground up. Paper realize given solutions with two main modules. RGB-net and $\\alpha$-net models. For each of the problem type, these models quantity(1 RGB-net for relight, 2 RGB-net for segmentation) and purpose change. \n",
    "\n",
    "These models harness the information that is obtained by flow between the frames. Inclusion of optical flow captures motion effectively and makes the model significantly moer resilient to variations in lighting.\n",
    "\n",
    "## Modules Overview\n",
    "\n",
    "**RGBnet:** Given that we only optimize the weights over a single video, a shallow convolutional U-Net is sufficent for the task. This model takes $X_t$ of the video seq. and outputs RGB layer. \n",
    "\n",
    "**$\\alpha$ Layer:** Similar to RGBNet arcitecture is again shallow U-Net for predicting the t-maps or opacity layer. This layer takes RGB representation of the forward optical flow($F^{RGB}_{t\\rightarrow t-1}$) \n",
    "\n",
    "## Video Relighting\n",
    "\n",
    "<center>\n",
    "    <figure>\n",
    "        <img src=\"figures/figure-1.png\" alt=\"Video Relighting\" title=\"Figure 1\" width=\"1000\">\n",
    "        <figcaption>Figure 1: Video Relighting</figcaption>\n",
    "    </figure>\n",
    "</center>\n",
    "\n",
    "$F^{(1)}_{RGB}$, $F^{(1)}_{\\alpha}$, $\\gamma^{-1}$ are optimized with the following loss objectives(below are general definition of the losses. Each module updates these a little)\n",
    "\n",
    "**Overall Loss Objective:** $L_{final}$ = $\\lambda_{rec}$ $L_{rec}$ + $\\lambda_{warp}$ $L_{warp}$ (1)\n",
    "\n",
    "**Reconstruction Loss:** $\\sum_t ||X_t - \\hat{X_t}||_1 + || \\phi (X_t) - \\phi (\\hat{X_t})||_1$ (2)\n",
    "\n",
    "**Optical Flow Warp Loss** $\\sum_t || F_{t-1 \\rightarrow t} (X_{t-1}^o) -  X_{t}^o  || $ (3)\n",
    "\n",
    "\n",
    "\n",
    "Relit video is reconstructed with the following equation.\n",
    "\n",
    "$X_t^{out} = A_t * (X_t^{in})^{\\gamma}$,  $\\forall t \\in (1,T] $ (4)\n",
    "\n",
    "\n",
    "For the VDP framework authors update eq. 4 as follows\n",
    "\n",
    "$log(X_t^{in}) = \\gamma^{-1}(log(1/A_t)+log(x_t^{out}))$, $\\forall t \\in (1,T] $ (5)\n",
    "\n",
    "\n",
    "Relighting task is evaluated on SDSD dataset where the video has relit and dark version of these. SSIM and PSNR metrics are utilized in order to evaluate quantatively.\n",
    "\n",
    "## Unsupervised Video Object Segmentation\n",
    "\n",
    "## Video Dehazing\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
